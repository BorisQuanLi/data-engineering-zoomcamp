{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "data-ingestion-approaches",
   "metadata": {},
   "source": [
    "# Data Ingestion Approaches\n",
    "\n",
    "## Approach 1: Using `pyarrow` to read Parquet and `pandas` to write to SQL in chunks\n",
    "\n",
    "**Advantages:**\n",
    "- Directly reads Parquet files, which is efficient for handling large datasets.\n",
    "- Allows for chunked processing, which can help manage memory usage and improve performance.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Requires converting the Parquet data to a Pandas DataFrame before writing to SQL, which can be memory-intensive for very large datasets.\n",
    "\n",
    "## Approach 2: Using `pandas.read_csv` with `iterator=True` and `chunksize`\n",
    "\n",
    "**Advantages:**\n",
    "- Efficient for reading large CSV files in chunks, which helps manage memory usage.\n",
    "- Allows for chunked processing, which can improve performance and avoid memory issues.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Only applicable to CSV files, not Parquet files. If your data is in Parquet format, you would need to convert it to CSV first.\n",
    "\n",
    "## Example of Approach 2\n",
    "\n",
    "Here is an example of how to use `pandas.read_csv` with `iterator=True` and `chunksize` to read a CSV file and write it to SQL in chunks:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(\"postgresql://postgres:root@localhost:5433/ny_taxi\")\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "csv_file = \"yellow_tripdata_2024-10.csv\"\n",
    "chunksize = 100000\n",
    "csv_iterator = pd.read_csv(csv_file, iterator=True, chunksize=chunksize)\n",
    "\n",
    "# Iterate over the chunks and write to SQL\n",
    "for chunk in csv_iterator:\n",
    "    chunk.to_sql(\"yellow_taxi_data\", con=engine, if_exists=\"append\")\n",
    "    print(f\"Written chunk with {len(chunk)} rows\")\n",
    "```\n",
    "\n",
    "## Approach 3: Using Dask to read Parquet and write to SQL in chunks\n",
    "\n",
    "**Advantages:**\n",
    "- Scales to larger-than-memory datasets by breaking them into smaller partitions and processing them in parallel.\n",
    "- Integrates well with the existing Python ecosystem, including Pandas and SQLAlchemy.\n",
    "- Can improve performance by parallelizing operations and distributing the workload across multiple cores or machines.\n",
    "- Can distribute the workload across different nodes in the cloud, enabling efficient processing of large datasets in a distributed computing environment.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Requires additional dependencies and setup.\n",
    "\n",
    "## Example of Approach 3\n",
    "\n",
    "Here is an example of how to use Dask to read a Parquet file and write it to SQL in chunks:\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(\"postgresql://postgres:root@localhost:5433/ny_taxi\")\n",
    "\n",
    "# Read the Parquet file using Dask\n",
    "dask_df = dd.read_parquet(\"yellow_tripdata_2024-10.parquet\")\n",
    "\n",
    "# Convert Dask DataFrame to Pandas DataFrame in chunks and write to SQL\n",
    "for chunk in dask_df.to_delayed():\n",
    "    chunk_df = chunk.compute()\n",
    "    chunk_df.to_sql(\"yellow_taxi_data\", con=engine, if_exists=\"append\")\n",
    "    print(f\"Written chunk with {len(chunk_df)} rows\")\n",
    "```\n",
    "\n",
    "## Checking Memory Size\n",
    "\n",
    "You can find out the memory size of the machine that processes the dataset using various methods depending on the operating system.\n",
    "\n",
    "### On Linux and macOS\n",
    "\n",
    "You can use the `free` command to check the memory size:\n",
    "\n",
    "```bash\n",
    "free -h\n",
    "```\n",
    "\n",
    "This command will display the total, used, and available memory in a human-readable format.\n",
    "\n",
    "### On Windows\n",
    "\n",
    "You can use the `systeminfo` command to check the memory size:\n",
    "\n",
    "```powershell\n",
    "systeminfo | findstr /C:\"Total Physical Memory\"\n",
    "```\n",
    "\n",
    "This command will display the total physical memory of the machine.\n",
    "\n",
    "### Using Python\n",
    "\n",
    "You can also use Python to check the memory size using the `psutil` library:\n",
    "\n",
    "1. **Install `psutil`:**\n",
    "\n",
    "```bash\n",
    "pip install psutil\n",
    "```\n",
    "\n",
    "2. **Check the memory size using Python:**\n",
    "\n",
    "```python\n",
    "import psutil\n",
    "\n",
    "# Get the total memory in bytes\n",
    "total_memory = psutil.virtual_memory().total\n",
    "\n",
    "# Convert to gigabytes\n",
    "total_memory_gb = total_memory / (1024 ** 3)\n",
    "\n",
    "print(f\"Total memory: {total_memory_gb:.2f} GB\")\n",
    "```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "- If your data is in Parquet format, the first approach using `pyarrow` and `pandas` is more suitable.\n",
    "- If your data is in CSV format, the second approach using `pandas.read_csv` with `iterator=True` and `chunksize` is more efficient.\n",
    "- If you need to handle larger-than-memory datasets or require parallel processing, the third approach using Dask is recommended.\n",
    "\n",
    "Choose the approach that best fits your data format and processing requirements. If you need to handle both CSV and Parquet formats, you can implement multiple approaches and use the appropriate one based on the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002bd0da-a1b4-45e0-84c3-0a6fca2bf4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4d15756-db8a-459c-9a78-f2e25bcf6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nyc.gov/assets/tlc/downloads/pdf/working_parquet_format.pdf\n",
    "trips = pq.read_table(\"yellow_tripdata_2024-10.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34fefcee-5af3-4097-928b-eeb843f525c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df = trips.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2777a5-7846-44f8-bda2-f7d18eb08e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                          int32\n",
       "tpep_pickup_datetime     datetime64[us]\n",
       "tpep_dropoff_datetime    datetime64[us]\n",
       "passenger_count                 float64\n",
       "trip_distance                   float64\n",
       "RatecodeID                      float64\n",
       "store_and_fwd_flag               object\n",
       "PULocationID                      int32\n",
       "DOLocationID                      int32\n",
       "payment_type                      int64\n",
       "fare_amount                     float64\n",
       "extra                           float64\n",
       "mta_tax                         float64\n",
       "tip_amount                      float64\n",
       "tolls_amount                    float64\n",
       "improvement_surcharge           float64\n",
       "total_amount                    float64\n",
       "congestion_surcharge            float64\n",
       "Airport_fee                     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# follow along with the Zoomcamp lecture\n",
    "# https://www.youtube.com/watch?v=2JM-ziJt0WI&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=7\n",
    "trips_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fce043ed-b97f-432d-bc65-b808ab5ed64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"postgresql://postgres:root@localhost:5433/ny_taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b60eb358-0aaf-45ae-953b-daaea6b7974e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" INTEGER, \n",
      "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count FLOAT(53), \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" FLOAT(53), \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" INTEGER, \n",
      "\t\"DOLocationID\" INTEGER, \n",
      "\tpayment_type BIGINT, \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53), \n",
      "\t\"Airport_fee\" FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write the trips dataframe into a db table\n",
    "print(pd.io.sql.get_schema(trips_df, name='yellow_taxi_data', con=engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4d72f62-d668-447a-8577-0da0e855e6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a db table that includes column names only\n",
    "trips_df.head(n=0).to_sql(\"yellow_taxi_data\", con=engine, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "061e8765-e59c-44e6-8e9b-f33f71f5860c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3833771, 19), 383, 3771)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of 10,000-row chunks to iterate over while writing the values to the database\n",
    "df_dim, iterations, last_batch = trips_df.shape, trips_df.shape[0]//10000, trips_df.shape[0]%10000\n",
    "df_dim, iterations, last_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05152b9-5976-4811-8f1e-80908b3bf983",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.head(10), trips_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3bb33-9e22-4a6e-801a-9913643bb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10000  # Define the chunk size\n",
    "for chunk in range(0, len(trips_df), chunksize):\n",
    "    %time trips_df.iloc[chunk:chunk + chunksize].to_sql(\"yellow_taxi_data\", con=engine, if_exists=\"append\")\n",
    "    print(f\"Written rows {chunk} to {chunk + chunksize}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
